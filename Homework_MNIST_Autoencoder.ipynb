{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAWVbhB16p-U"
      },
      "source": [
        "# Homework: Autoencoding MNIST (and Celebrity Faces)\n",
        "\n",
        "\n",
        "> **Due Date: February 27th, 2026 @ 1:00pm**\n",
        ">\n",
        "> Please turn in this completed notebook via Google classroom. Slack or email Alex or Vishvak if you run into any issues.\n",
        "\n",
        "**Collaboration policy and more**\n",
        "\n",
        "You're welcome (and highly encouraged) to work with and discuss this homework assignment with others in the class, and feel free to use any resources (textbooks, online notebooks, etc). The only requirement is that the final notebook that you turn in must be your own written work (no copy and pasting, please).\n",
        "\n",
        "**Overview**\n",
        "\n",
        "In class, we cover how Hinton and Salakhutdinov's 2006 Science Paper, [\"Reducing the Dimensionality of Data with Neural Networks\"](https://www.science.org/doi/10.1126/science.1127647) was one of the first demonstrations of unsupervised pretraining for use in training deep neural networks. In this homework, we'll implement autoencoders in the context of MNIST. Additionally, as an optional assignment, a similar architecture can be used for a subset of CelebA dataset of celebrity faces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px8y6lmq6p-V"
      },
      "source": [
        "## Before you get started\n",
        "\n",
        "**1) Background reading**\n",
        "\n",
        "Please Read Hinton and Salakhutdinov's 2006 seminal work on deep autoencoders (https://www.science.org/doi/10.1126/science.1127647), as this notebook aims to recreate this important work. A few questions to think about as you read that will help you in this assignment:\n",
        "  - What architecture do they use for their deep autoencoders?\n",
        "  - Why were deep neural networks so much harder to train in 2006?\n",
        "\n",
        "**2) How to run this notebook**\n",
        "\n",
        "This Jupyter Notebook can be used in two ways:\n",
        "* *Option 1: Download the notebook*\n",
        "\n",
        "  We've included all the imports necessary for this homework. Please make sure you're running Python 3 with PyTorch (and Torchvision) installed and ready to go, along with NumPy and Matplotlib. Although you might find that these models train a bit faster on GPU, this homework assignment should be doable on most modern laptops. If you're having trouble please let us know ASAP.\n",
        "\n",
        "* *Option 2: Run it online on Google Colaboratory*\n",
        "\n",
        "  - Colab gives access to a GPU, so it could be useful in case you don't have CUDA installed on your computer (**Note: you can use this as an opportunity to get started on GPU training, but we recommend you develop your model and make sure everything works on CPU first**)\n",
        "  - Make a copy of this notebook in your Google Drive folder: \"File\" -> \"Save a copy in Drive...\"\n",
        "  - By default, Colab does not make GPUs available, but you can easily access them by selecting GPU in \"Runtime\" -> \"Change runtime type...\"\n",
        "  - Remember that Colab runs in a temporary virtual machine, so all the data created while running the notebook will be lost at the end of the session, or when the runtime disconnects due to inactivity. To preserve data between sessions, there are a couple of options:\n",
        "    * you can link Colab to your personal Google Drive by mounting it on your runtime, see first cell below.\n",
        "    * you can download/upload files from the Files tab on the right sidebar.\n",
        "\n",
        "**3) How to complete this assignment**\n",
        "\n",
        "  - Fill out the relevant code blocks as indicated\n",
        "  - Answer questions by writing them directly in the text block. Please keep your written answers concise, most are meant to be answered in a sentence or two.\n",
        "  - Make figures showing your results and add comments with your observations.\n",
        "\n",
        "**4) Optional exercise: CelebA Data**\n",
        "\n",
        "Whereas MNIST is a toy dataset built into PyTorch, we can also examine a more complex feature space using a subset of 90,000 celebrity portraits from CelebA (see [Liu et al. (2014), \"Deep Learning Face Attributes in the Wild\"](https://arxiv.org/abs/1411.7766)). This is an optional part of the homework, but is a nice way to see how autoencoders perform on other types of visual data. There will be a .zip file of the relevant celebrity faces dataset on the Google Classroom link.\n",
        "\n",
        "***Let's start!***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykN_eZdJ6p-f"
      },
      "source": [
        "## Train an autoencoder on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzyVPwhkMWk3"
      },
      "source": [
        "The following command can be used to mount your personal Google Drive folder on the temporary virtual machine, so you can recover data between sessions (follow the instructions, you'll need an authorization code). Additional info [here](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "STvHSt3zICjF"
      },
      "outputs": [],
      "source": [
        "# # Skip this cell if running locally\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "fgkNXdEr6p-X"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Import all the necessary libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es5SJPnnVISe"
      },
      "source": [
        "Using CUDA will make training be faster, but if you are working on your own computer without CUDA you can set `use_cuda = False` below.\n",
        "\n",
        "If you are working on Colab, make sure to use a runtime with GPU (\"Runtime\" -> \"Change runtime type\" -> \"{one of H100, A100, L4, T4} GPU\").\n",
        "\n",
        "\n",
        "You can check if CUDA is available on your local computer with: `torch.cuda.is_available()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKT2zNJR6Rwj",
        "outputId": "0d1b213b-7220-40fe-efa0-cdae30e47a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "28wNsWmC6Q63"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(168);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBrzn_tnFKsM"
      },
      "source": [
        "> **Question 0.1) Why is it important to set the seed for the random number generator?**\n",
        "    np.random.seed(random_seed) takes the parameter value as the seed, then create a fixed set of \"random\" numbers every time based on the same random_seed. However, where the parameters is left blank, np.random.seed() set the seed to a random number, resulting in different set of random numbers. It is important to set the seed for the random number generator when the user would like the \"random numbers\" to be predictable, making it easier for debugging the code with them fixed.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUNoP7qY6p-g"
      },
      "source": [
        "### 1. MNIST Dataset\n",
        "\n",
        "As noted in class, MNIST has been widely used to benchmark new deep learning architectures and is already built into PyTorch. We provide this data as a starting point, again noting that the mean and std of the training set are calculated to be 0.1307 and 0.3081, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H20Wcmu6p-h",
        "outputId": "7c01dcc3-88d9-451c-8ad5-eb5805f7880b"
      },
      "outputs": [],
      "source": [
        "preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    './bmi212_downloads', train=True, download=True,\n",
        "    transform=preprocessing)\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    './bmi212_downloads', train=False, download=True,\n",
        "    transform=preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oay1ii680sfO",
        "outputId": "6d2e9904-1c15-4dff-a3cf-a7acde90b62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0][0].size())\n",
        "print(test_dataset[0][0].size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(len(test_dataset))\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPPEsT96p-l"
      },
      "source": [
        "> **Q1.1) How many examples do the training set and test set have?**\n",
        "    training: 60,000. test: 10,000.\n",
        "...\n",
        "\n",
        "> **Q1.2) What's the format of each input example? Can we directly put these into a fully-connected layer?**\n",
        "    Each input sample is a 1 by 28 by 28 picture of 28*28 (784) pixels. We cannot directly feed this into a fully-connected layer because it expects a 1D feature vector. The image must first be flattened into a 784-dimensional vector. \n",
        "...\n",
        "\n",
        "> **Q1.3) Why do we normalize the input data for neural networks?**\n",
        "    We normalize input data to ensure stable and faster training by keeping feature values in a similar scale, preventing large gradients, and improving optimization efficiency.\n",
        "...\n",
        "\n",
        "> **Q1.4) In this scenario, MNIST is already split into a training set and a test set. What is the purpose of dataset splitting (and specifically, the purpose of a test set)? For modern deep learning, a three-way split into training, validation, and test sets is usually preferred, why?**\n",
        "    Dataset splitting ensures that we evaluate a modelâ€™s ability to generalize to unseen data. The training set is used to learn model parameters, while the test set provides an unbiased evaluation of final model performance.\n",
        "    In modern deep learning, a validation set is introduced to tune hyperparameters and select models without contaminating the test set. This preserves the test set as a truly independent benchmark for final evaluation.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr83wABJ6p-m"
      },
      "source": [
        "### 2. Using DataLoaders for MNIST\n",
        "\n",
        "Set up the DataLoader objects below. Although the arguments are prepopulated, you may need to change the batch sizes or other arguments during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Df46Ok2t6p-n"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 16  # <-- Please change this as necessary\n",
        "NUM_WORKERS = 2  # <-- Use more workers for more CPU threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "Tx21NwCGqwE-"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvgrdGuhdwKt",
        "outputId": "80e07c3b-dd8e-4d56-ca6a-07600dfe5465"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([16, 1, 28, 28])"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_batch=next(iter(train_loader))\n",
        "one_batch[0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jL6Zs5z6p-r"
      },
      "source": [
        "> **Q2.1) It's recommended to shuffle the training data over each epoch, but this isn't typically the case for the test set, why?**\n",
        "    By reshuffling each epoch, we ensure different sample groupings, providing better stochasticity and improving optimization stability. Test data does not require shuffling because no learning occurs during evaluation, and performance metrics are independent of sample order. \n",
        "...\n",
        "\n",
        "> **Q2.2) What seems to be a good batch size for training? What happens if you train with a batch size of 1? What about a batch size equal to the total training set?**\n",
        "    A good batch size balances computational efficiency and gradient stability. Common values are 32 or 64. With batch size = 1, training becomes fully stochastic, resulting in highly noisy gradient updates and unstable training. With batch size equal to the entire training set, the gradient is exact but updates are infrequent and computationally expensive, which may reduce generalization performance.\n",
        "...\n",
        "\n",
        "> **Q2.3) The PyTorch DataLoader object is an iterator that generates batches as it's called. Try to pull a few images from the training set to see what these images look like. Does the DataLoader return only the images? What about the labels?**\n",
        "    The DataLoader returns both images and labels as a tuple `(images, labels)`. Each batch contains tensors of shape `[16, 1, 28, 28]` for images and [16] for labels. \n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16, 1, 28, 28])\n",
            "torch.Size([16])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(1,4,i+1)\n",
        "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
        "    plt.title(labels[i].item())\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMlogSbL6p-s"
      },
      "source": [
        "### 3. Define your neural network architecture\n",
        "\n",
        "With your data and dataloaders appropriately set, you're ready to define a network architecture. In this homework, we'll ask you to evaluate two different architectures.\n",
        "\n",
        "For the first (we'll call it `HNet` in this homework), please implement Hinton's 2006 architecture of 7-hidden layers:\n",
        "\n",
        "```[784 x 1000 x 500 x 250 x 2 x 250 x 500 x 1000 x 784]. ```\n",
        "\n",
        "For the second, implement your own autoencoder architecture, `MyNet`, again using a bottleneck dimension of 2. As a note, the larger your model, the longer it will take to train. Can you achieve similar performance to the model above using a more condensed model?\n",
        "\n",
        "**Tips:**\n",
        "* Try different activation functions (Tanh, Sigmoid, ReLU, etc)\n",
        "* A sequence of layers can be defined more easily using `nn.Sequential`, see [docs](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
        "* Split your network into an `.encode()` and a `.decode()`, that will be called sequentially in `.forward()`. This will be useful later on when we'll ask to visualize the low-dimensional embeddings (\"latent space\") produced by the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGK2j8UM6p-t"
      },
      "outputs": [],
      "source": [
        "class HNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ### Implement a version of Hinton's 2006 Autoencoder,\n",
        "        ### using a bottleneck latent dimension of 2\n",
        "        self.encoder = nn.Sequential(\n",
        "            # implement the encoder architecture\n",
        "                nn.Linear(784, 1000),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(1000, 500),\n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(500, 250),   \n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(250, 2)  \n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # implement the decoder architecture\n",
        "                nn.Linear(2, 250),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(250, 500),\n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(500, 1000),   \n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(1000, 784), \n",
        "                nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        ### Implement the encoder forward pass\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        ### Implement the decoder forward pass\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Implement the complete forward pass\n",
        "        if x.dim() == 4:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        z = self.encode(x)\n",
        "        recon = self.decode(z)\n",
        "        # Reshape to image shape: [B,1,28,28]\n",
        "        recon_img = recon.view(x.size(0), 1, 28, 28)\n",
        "        return recon_img, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "O_Grvigg6p-x"
      },
      "outputs": [],
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ### Fill in your network architecture here\n",
        "        ### also using a bottleneck latent dimension of 2\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # implement your decoder architecture\n",
        "            nn.Linear(2, 100),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(100, 400),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(400, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        ### Implement the encoder forward pass\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        ### Implement the decoder forward pass\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Implement the complete forward pass\n",
        "        if x.dim() == 4:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        z = self.encode(x)\n",
        "        out = self.decode(z)\n",
        "        out = out.view(x.size(0), 1, 28, 28)\n",
        "        return out, z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQ-CAnlYad2"
      },
      "source": [
        "> **Q3.1) What activation functions did you use, and why?**\n",
        "    I used ReLU activation functions in the hidden layers of both the encoder and decoder because ReLU helps avoid vanishing gradients and improves training stability. For the final output layer, I used a Sigmoid activation to constrain the reconstructed pixel values to the range [0,1], matching the normalized MNIST input data.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2rGw2FD6p-2"
      },
      "source": [
        "### 4. Write your own training function\n",
        "\n",
        "Write your own training function that takes your **model**, an **optimizer**, and a **training criterion**, and iterates over the **training set**.\n",
        "* *Hint*: Because an autoencoder is a form of unsupervised learning, we won't need to use the labels like in the MNIST classification example. Keep in mind the format of the images and whether they're compatible with feed-forward networks.\n",
        "* For each epoch, print and record (in an array or list) the training loss.\n",
        "* You may want to save the model and its weights on file at regular intervals ([checkpointing](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)). In order to visualize the autoencoder's learning process, we suggest to save at least three timepoints: early, intermediate, and final (for instance, if your model converges after 60 epochs, save your model at 5 epochs, 30 epochs, and 60 epochs).\n",
        "\n",
        "A few useful tips:\n",
        "- Feel free to look at the MNIST classification notebook from previous recitations and use it as a template.\n",
        "- Printing out the intermediate variables and their shape at each step can be helpful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o2vn3O8L6p-2"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, optimizer, criterion,\n",
        "          n_epochs=10, **kwargs):\n",
        "\n",
        "    ### Define your training loop here\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsMeeItp6p-6"
      },
      "source": [
        "### 5. Define your optimization and evaluation criterion\n",
        "\n",
        "Define an optimizer and criterion (loss function) for your neural network training. To setup your optimizer, you'll have to instantiate your models above, and choose a learning rate. Try a few different optimizers and learning rates to get a sense of what will train within a reasonable timeframe (if your deep network isn't too deep, reaching convergence shouldn't take more than 5-10 minutes with the right choice of learning rate and optimizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQBWlyJcj7w"
      },
      "source": [
        "> **Q5.1) What loss function is suited to this problem?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q5.2) Try a few optimizers, what seemed to work best?**\n",
        "\n",
        "...\n",
        "\n",
        "> **Q5.3) What's the effect of choosing different batch sizes?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ftxeVJJP6p-7"
      },
      "outputs": [],
      "source": [
        "### Instantiate your model\n",
        "### Define your loss function (training criterion)\n",
        "### Choose your optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zudx4FKI6p_C"
      },
      "source": [
        "### 6. Run your training loop\n",
        "\n",
        "It's a great idea to monitor the early epochs of your training (\"babysit your training\") to keep an eye on learning. Does the learning rate seem too high? too low?\n",
        "\n",
        "(**Hint: it's recommended that you just test a single epoch at a time while you write your training function, to debug and make sure everything is working appropriately.**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J87zUaXX6p_D"
      },
      "outputs": [],
      "source": [
        "### Set a number of training epochs and train your model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoOJQt6vFADr"
      },
      "source": [
        "In your training loop, we requested that you store your training loss for each epoch. Using Matplotlib, please plot your training loss as a function of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xycLVQK-jwiB"
      },
      "outputs": [],
      "source": [
        "### Plot loss curve using Matplotlib"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitAd61L6p_M"
      },
      "source": [
        "> **Q6.1)  How do you know when your network is done training?**\n",
        "\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxomi9w5kxIx"
      },
      "source": [
        "Another way to check if your models (`HNet` and `MyNet`) are well trained is to plot a few image reconstructions to see how well your models do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pjS79M0oDj_"
      },
      "outputs": [],
      "source": [
        "# extract 6 figures from training DataLoader\n",
        "mini_batch, _ = next(iter(train_loader))\n",
        "n_examples = min(6, mini_batch.shape[0])\n",
        "examples = mini_batch[:n_examples]\n",
        "\n",
        "# compute reconstructions\n",
        "with torch.no_grad():\n",
        "    reconstr_examples = model.forward(\n",
        "        examples.view(n_examples, -1).to(device))\n",
        "\n",
        "# save image with original v. reconstructed images\n",
        "comparison = torch.cat([\n",
        "    examples,\n",
        "    reconstr_examples.view(-1, 1, 28, 28).cpu()])\n",
        "save_image(comparison.cpu(), 'training_reconstruction.png', nrow=n_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XG_F4XTqwk9"
      },
      "outputs": [],
      "source": [
        "Image('training_reconstruction.png', width=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNZwHPUGxKu"
      },
      "source": [
        "> **Q6.2) What does `torch.no_grad()` do?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC8PXHyR6p_Q"
      },
      "source": [
        "### 7. Visualize the learning process\n",
        "\n",
        "We'll next try to visualize how well the model is learning on the **test set**. To do this, we'll first visualize the \"learning process\" by viewing reconstruction at various stages.\n",
        "\n",
        "* Using your checkpoints saved during training, plot a batch of images from the test set and their corresponding reconstructions based on each of your saved models over time. You should see the quality of the reconstructions improving over time.\n",
        "* To visualize images, you can use the helper functions provided below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LDO9E6Vl6p_N"
      },
      "outputs": [],
      "source": [
        "### Helper Functions for Plotting Multiple Images\n",
        "\n",
        "def imshow(inp,\n",
        "           figsize=(10,10),\n",
        "           mean=0.1307, # for MNIST train\n",
        "           std=0.3081, # for MNIST train\n",
        "           title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.cpu().detach()\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array(mean)\n",
        "    std = np.array(std)\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "def reconstructions_from_batch(model, batch):\n",
        "    batch = batch.view(-1, 28 * 28).to(device)\n",
        "    reconstruction = model(batch)\n",
        "    return reconstruction.reshape(batch.shape[0],1,28,28)\n",
        "\n",
        "# Get a batch of training data\n",
        "batch, classes = next(iter(test_loader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(batch)\n",
        "imshow(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E2edUEVG3Rev"
      },
      "outputs": [],
      "source": [
        "### Iterate over checkpoints and plot reconstruction\n",
        "### figures from the test set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhuSg4D6p_R"
      },
      "source": [
        "### 8. Visualize the latent space\n",
        "\n",
        "As discussed in class, the first half of an autoencoder (the *encoder*) maps the original input into a lower-dimensional latent space.\n",
        "* Just as shown in Hinton and Salakhutdinov, run your test set of 10,000 MNIST digits through the **encoding layer** of one of the trained networks above. Each sample should readily map to a 2-dimension point. To do this, it will be helpful to fill out a new function, **encode** below, that takes in your trained model and the `test_dataloader` to produce 2d latent embeddings and their corresponding labels.\n",
        "* Plot each point in these two dimensions, and color each point in this **latent space** by their known **labels**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PNg0PfrK4gzn"
      },
      "outputs": [],
      "source": [
        "### Write a helper function to grab examples from the test_loader to generate\n",
        "### pairs of embeddings and their associated labels\n",
        "\n",
        "def get_encodings(model, device, test_loader):\n",
        "  #### Fill this in! ####\n",
        "  latent_embeddings = None # get the latent embeddings from the bottleneck\n",
        "  labels = None # each pair of coordinates has an associated label\n",
        "  return latent_embeddings, labels\n",
        "\n",
        "### Plot 2D latent space representation color-coded according to their true labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGkujKgGPEi"
      },
      "source": [
        "> **Q8.1) Does your autoencoder separate out different classes effectively? What classes seem to be closer and what classes are farther apart in this latent space?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78___-BU6p_S"
      },
      "source": [
        "## Optional (advanced): Train an autoencoder on CelebA Faces\n",
        "\n",
        "Real-world images tend to be far more complex than digits from MNIST. As an optional exercise for your own interest, or for students looking for more experience, we'll investigate a subset of CelebA below.\n",
        "\n",
        "We provide the images in a .zip file (`faces.zip`) in the class's Google Drive folder, which contains a \"train\" and \"test\" set of 80k and 10k images, respectively. Although these are color, RGB images, below we've set up the datasets to convert these to grayscale with precomputed means (0.4401) and stds (0.2407), for convenience and easier compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1K6o1Wr88EXj"
      },
      "outputs": [],
      "source": [
        "### Download faces.zip and unzip it into bmi219_downloads/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nM7WXmAu6p_T"
      },
      "outputs": [],
      "source": [
        "# preprocessing = transforms.Compose([\n",
        "#     transforms.Grayscale(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.4401,), (0.2407,)),\n",
        "# ])\n",
        "\n",
        "# train_dataset = datasets.ImageFolder(\n",
        "#     'bmi212_downloads/Faces/train',\n",
        "#     transform=preprocessing)\n",
        "\n",
        "# test_dataset = datasets.ImageFolder(\n",
        "#     'bmi212_downloads/Faces/test',\n",
        "#     transform=preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQn4cgBW6p_X"
      },
      "source": [
        "As above, you'll want to:\n",
        "\n",
        "1. set up your dataloaders and visualize some of the images\n",
        "2. set up your autoencoder network architecture\n",
        "3. define your training criterion and optimizer\n",
        "4. train your network\n",
        "    \n",
        "In this case, you should be able to reuse much of your code from above. Consider a few questions:\n",
        "\n",
        "1. How well do complex images like faces work with a latent dimension of 2?\n",
        "2. Do reconstructions look better with a larger bottleneck?\n",
        "3. What kind of features are poorly reconstructed? What happens to sunglasses, hats, and hands?\n",
        "4. Try sampling the 2-d latent space close to existing examples (by adding some noise...) or randomly. What do the generated images look like?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "homework",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
