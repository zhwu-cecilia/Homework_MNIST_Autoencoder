{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAWVbhB16p-U"
      },
      "source": [
        "# Homework: Autoencoding MNIST (and Celebrity Faces)\n",
        "\n",
        "\n",
        "> **Due Date: February 27th, 2026 @ 1:00pm**\n",
        ">\n",
        "> Please turn in this completed notebook via Google classroom. Slack or email Alex or Vishvak if you run into any issues.\n",
        "\n",
        "**Collaboration policy and more**\n",
        "\n",
        "You're welcome (and highly encouraged) to work with and discuss this homework assignment with others in the class, and feel free to use any resources (textbooks, online notebooks, etc). The only requirement is that the final notebook that you turn in must be your own written work (no copy and pasting, please).\n",
        "\n",
        "**Overview**\n",
        "\n",
        "In class, we cover how Hinton and Salakhutdinov's 2006 Science Paper, [\"Reducing the Dimensionality of Data with Neural Networks\"](https://www.science.org/doi/10.1126/science.1127647) was one of the first demonstrations of unsupervised pretraining for use in training deep neural networks. In this homework, we'll implement autoencoders in the context of MNIST. Additionally, as an optional assignment, a similar architecture can be used for a subset of CelebA dataset of celebrity faces.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px8y6lmq6p-V"
      },
      "source": [
        "## Before you get started\n",
        "\n",
        "**1) Background reading**\n",
        "\n",
        "Please Read Hinton and Salakhutdinov's 2006 seminal work on deep autoencoders (https://www.science.org/doi/10.1126/science.1127647), as this notebook aims to recreate this important work. A few questions to think about as you read that will help you in this assignment:\n",
        "  - What architecture do they use for their deep autoencoders?\n",
        "  - Why were deep neural networks so much harder to train in 2006?\n",
        "\n",
        "**2) How to run this notebook**\n",
        "\n",
        "This Jupyter Notebook can be used in two ways:\n",
        "* *Option 1: Download the notebook*\n",
        "\n",
        "  We've included all the imports necessary for this homework. Please make sure you're running Python 3 with PyTorch (and Torchvision) installed and ready to go, along with NumPy and Matplotlib. Although you might find that these models train a bit faster on GPU, this homework assignment should be doable on most modern laptops. If you're having trouble please let us know ASAP.\n",
        "\n",
        "* *Option 2: Run it online on Google Colaboratory*\n",
        "\n",
        "  - Colab gives access to a GPU, so it could be useful in case you don't have CUDA installed on your computer (**Note: you can use this as an opportunity to get started on GPU training, but we recommend you develop your model and make sure everything works on CPU first**)\n",
        "  - Make a copy of this notebook in your Google Drive folder: \"File\" -> \"Save a copy in Drive...\"\n",
        "  - By default, Colab does not make GPUs available, but you can easily access them by selecting GPU in \"Runtime\" -> \"Change runtime type...\"\n",
        "  - Remember that Colab runs in a temporary virtual machine, so all the data created while running the notebook will be lost at the end of the session, or when the runtime disconnects due to inactivity. To preserve data between sessions, there are a couple of options:\n",
        "    * you can link Colab to your personal Google Drive by mounting it on your runtime, see first cell below.\n",
        "    * you can download/upload files from the Files tab on the right sidebar.\n",
        "\n",
        "**3) How to complete this assignment**\n",
        "\n",
        "  - Fill out the relevant code blocks as indicated\n",
        "  - Answer questions by writing them directly in the text block. Please keep your written answers concise, most are meant to be answered in a sentence or two.\n",
        "  - Make figures showing your results and add comments with your observations.\n",
        "\n",
        "**4) Optional exercise: CelebA Data**\n",
        "\n",
        "Whereas MNIST is a toy dataset built into PyTorch, we can also examine a more complex feature space using a subset of 90,000 celebrity portraits from CelebA (see [Liu et al. (2014), \"Deep Learning Face Attributes in the Wild\"](https://arxiv.org/abs/1411.7766)). This is an optional part of the homework, but is a nice way to see how autoencoders perform on other types of visual data. There will be a .zip file of the relevant celebrity faces dataset on the Google Classroom link.\n",
        "\n",
        "***Let's start!***\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ykN_eZdJ6p-f"
      },
      "source": [
        "## Train an autoencoder on MNIST"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzyVPwhkMWk3"
      },
      "source": [
        "The following command can be used to mount your personal Google Drive folder on the temporary virtual machine, so you can recover data between sessions (follow the instructions, you'll need an authorization code). Additional info [here](https://colab.research.google.com/notebooks/io.ipynb#scrollTo=u22w3BFiOveA).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "STvHSt3zICjF"
      },
      "outputs": [],
      "source": [
        "# # Skip this cell if running locally\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fgkNXdEr6p-X"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# Import all the necessary libraries\n",
        "import numpy as np\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "es5SJPnnVISe"
      },
      "source": [
        "Using CUDA will make training be faster, but if you are working on your own computer without CUDA you can set `use_cuda = False` below.\n",
        "\n",
        "If you are working on Colab, make sure to use a runtime with GPU (\"Runtime\" -> \"Change runtime type\" -> \"{one of H100, A100, L4, T4} GPU\").\n",
        "\n",
        "\n",
        "You can check if CUDA is available on your local computer with: `torch.cuda.is_available()`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKT2zNJR6Rwj",
        "outputId": "0d1b213b-7220-40fe-efa0-cdae30e47a54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cpu\n"
          ]
        }
      ],
      "source": [
        "use_cuda = False\n",
        "device = torch.device(\"cuda\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "28wNsWmC6Q63"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(168);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBrzn_tnFKsM"
      },
      "source": [
        "> **Question 0.1) Why is it important to set the seed for the random number generator?**\n",
        "    np.random.seed(random_seed) takes the parameter value as the seed, then create a fixed set of \"random\" numbers every time based on the same random_seed. However, where the parameters is left blank, np.random.seed() set the seed to a random number, resulting in different set of random numbers. It is important to set the seed for the random number generator when the user would like the \"random numbers\" to be predictable, making it easier for debugging the code with them fixed.   "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SUNoP7qY6p-g"
      },
      "source": [
        "### 1. MNIST Dataset\n",
        "\n",
        "As noted in class, MNIST has been widely used to benchmark new deep learning architectures and is already built into PyTorch. We provide this data as a starting point, again noting that the mean and std of the training set are calculated to be 0.1307 and 0.3081, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8H20Wcmu6p-h",
        "outputId": "7c01dcc3-88d9-451c-8ad5-eb5805f7880b"
      },
      "outputs": [],
      "source": [
        "preprocessing = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.1307,), (0.3081,))\n",
        "])\n",
        "\n",
        "train_dataset = datasets.MNIST(\n",
        "    './bmi212_downloads', train=True, download=True,\n",
        "    transform=preprocessing)\n",
        "\n",
        "test_dataset = datasets.MNIST(\n",
        "    './bmi212_downloads', train=False, download=True,\n",
        "    transform=preprocessing)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oay1ii680sfO",
        "outputId": "6d2e9904-1c15-4dff-a3cf-a7acde90b62f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([1, 28, 28])\n",
            "torch.Size([1, 28, 28])\n"
          ]
        }
      ],
      "source": [
        "print(train_dataset[0][0].size())\n",
        "print(test_dataset[0][0].size())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10000\n",
            "60000\n"
          ]
        }
      ],
      "source": [
        "print(len(test_dataset))\n",
        "print(len(train_dataset))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnPPEsT96p-l"
      },
      "source": [
        "> **Q1.1) How many examples do the training set and test set have?**\n",
        "    training: 60,000. test: 10,000.\n",
        "...\n",
        "\n",
        "> **Q1.2) What's the format of each input example? Can we directly put these into a fully-connected layer?**\n",
        "    Each input sample is a 1 by 28 by 28 picture of 28*28 (784) pixels. We cannot directly feed this into a fully-connected layer because it expects a 1D feature vector. The image must first be flattened into a 784-dimensional vector. \n",
        "...\n",
        "\n",
        "> **Q1.3) Why do we normalize the input data for neural networks?**\n",
        "    We normalize input data to ensure stable and faster training by keeping feature values in a similar scale, preventing large gradients, and improving optimization efficiency.\n",
        "...\n",
        "\n",
        "> **Q1.4) In this scenario, MNIST is already split into a training set and a test set. What is the purpose of dataset splitting (and specifically, the purpose of a test set)? For modern deep learning, a three-way split into training, validation, and test sets is usually preferred, why?**\n",
        "    Dataset splitting ensures that we evaluate a modelâ€™s ability to generalize to unseen data. The training set is used to learn model parameters, while the test set provides an unbiased evaluation of final model performance.\n",
        "    In modern deep learning, a validation set is introduced to tune hyperparameters and select models without contaminating the test set. This preserves the test set as a truly independent benchmark for final evaluation.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vr83wABJ6p-m"
      },
      "source": [
        "### 2. Using DataLoaders for MNIST\n",
        "\n",
        "Set up the DataLoader objects below. Although the arguments are prepopulated, you may need to change the batch sizes or other arguments during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Df46Ok2t6p-n"
      },
      "outputs": [],
      "source": [
        "BATCH_SIZE = 128  # <-- Please change this as necessary\n",
        "NUM_WORKERS = 1  # <-- Use more workers for more CPU threads"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Tx21NwCGqwE-"
      },
      "outputs": [],
      "source": [
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=True,\n",
        "    num_workers=NUM_WORKERS)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    test_dataset,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    shuffle=False,\n",
        "    num_workers=NUM_WORKERS)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvgrdGuhdwKt",
        "outputId": "80e07c3b-dd8e-4d56-ca6a-07600dfe5465"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "torch.Size([128, 1, 28, 28])"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "one_batch=next(iter(train_loader))\n",
        "one_batch[0].size()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jL6Zs5z6p-r"
      },
      "source": [
        "> **Q2.1) It's recommended to shuffle the training data over each epoch, but this isn't typically the case for the test set, why?**\n",
        "    By reshuffling each epoch, we ensure different sample groupings, providing better stochasticity and improving optimization stability. Test data does not require shuffling because no learning occurs during evaluation, and performance metrics are independent of sample order. \n",
        "...\n",
        "\n",
        "> **Q2.2) What seems to be a good batch size for training? What happens if you train with a batch size of 1? What about a batch size equal to the total training set?**\n",
        "    A good batch size balances computational efficiency and gradient stability. Common values are 32 or 64. With batch size = 1, training becomes fully stochastic, resulting in highly noisy gradient updates and unstable training. With batch size equal to the entire training set, the gradient is exact but updates are infrequent and computationally expensive, which may reduce generalization performance.\n",
        "...\n",
        "\n",
        "> **Q2.3) The PyTorch DataLoader object is an iterator that generates batches as it's called. Try to pull a few images from the training set to see what these images look like. Does the DataLoader return only the images? What about the labels?**\n",
        "    The DataLoader returns both images and labels as a tuple `(images, labels)`. Each batch contains tensors of shape `[16, 1, 28, 28]` for images and [16] for labels. \n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([128, 1, 28, 28])\n",
            "torch.Size([128])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 4 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "data_iter = iter(train_loader)\n",
        "images, labels = next(data_iter)\n",
        "print(images.shape)\n",
        "print(labels.shape)\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "for i in range(4):\n",
        "    plt.subplot(1,4,i+1)\n",
        "    plt.imshow(images[i].squeeze(), cmap='gray')\n",
        "    plt.title(labels[i].item())\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mMlogSbL6p-s"
      },
      "source": [
        "### 3. Define your neural network architecture\n",
        "\n",
        "With your data and dataloaders appropriately set, you're ready to define a network architecture. In this homework, we'll ask you to evaluate two different architectures.\n",
        "\n",
        "For the first (we'll call it `HNet` in this homework), please implement Hinton's 2006 architecture of 7-hidden layers:\n",
        "\n",
        "```[784 x 1000 x 500 x 250 x 2 x 250 x 500 x 1000 x 784]. ```\n",
        "\n",
        "For the second, implement your own autoencoder architecture, `MyNet`, again using a bottleneck dimension of 2. As a note, the larger your model, the longer it will take to train. Can you achieve similar performance to the model above using a more condensed model?\n",
        "\n",
        "**Tips:**\n",
        "* Try different activation functions (Tanh, Sigmoid, ReLU, etc)\n",
        "* A sequence of layers can be defined more easily using `nn.Sequential`, see [docs](https://pytorch.org/docs/stable/generated/torch.nn.Sequential.html#torch.nn.Sequential)\n",
        "* Split your network into an `.encode()` and a `.decode()`, that will be called sequentially in `.forward()`. This will be useful later on when we'll ask to visualize the low-dimensional embeddings (\"latent space\") produced by the encoder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "gGK2j8UM6p-t"
      },
      "outputs": [],
      "source": [
        "class HNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ### Implement a version of Hinton's 2006 Autoencoder,\n",
        "        ### using a bottleneck latent dimension of 2\n",
        "        self.encoder = nn.Sequential(\n",
        "            # implement the encoder architecture\n",
        "                nn.Linear(784, 1000),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(1000, 500),\n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(500, 250),   \n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(250, 2)  \n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # implement the decoder architecture\n",
        "                nn.Linear(2, 250),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(250, 500),\n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(500, 1000),   \n",
        "                nn.ReLU(),         \n",
        "                nn.Linear(1000, 784)\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        ### Implement the encoder forward pass\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        ### Implement the decoder forward pass\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Implement the complete forward pass\n",
        "        if x.dim() == 4:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        z = self.encode(x)\n",
        "        recon = self.decode(z)\n",
        "        # Reshape to image shape: [B,1,28,28]\n",
        "        recon_img = recon.view(x.size(0), 1, 28, 28)\n",
        "        return recon_img, z"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "O_Grvigg6p-x"
      },
      "outputs": [],
      "source": [
        "class MyNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ### Fill in your network architecture here\n",
        "        ### also using a bottleneck latent dimension of 2\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(784, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 2)\n",
        "        )\n",
        "\n",
        "        self.decoder = nn.Sequential(\n",
        "            # implement your decoder architecture\n",
        "            nn.Linear(2, 64),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(64, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, 784),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def encode(self, x):\n",
        "        ### Implement the encoder forward pass\n",
        "        return self.encoder(x)\n",
        "\n",
        "    def decode(self, x):\n",
        "        ### Implement the decoder forward pass\n",
        "        return self.decoder(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        ### Implement the complete forward pass\n",
        "        if x.dim() == 4:\n",
        "            x = x.view(x.size(0), -1)\n",
        "        z = self.encode(x)\n",
        "        out = self.decode(z)\n",
        "        out = out.view(x.size(0), 1, 28, 28)\n",
        "        return out, z"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oqQ-CAnlYad2"
      },
      "source": [
        "> **Q3.1) What activation functions did you use, and why?**\n",
        "    I used ReLU activation functions in the hidden layers of both the encoder and decoder because ReLU helps avoid vanishing gradients and improves training stability. For the final output layer, I used a Sigmoid activation to constrain the reconstructed pixel values to the range [0,1], matching the normalized MNIST input data.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2rGw2FD6p-2"
      },
      "source": [
        "### 4. Write your own training function\n",
        "\n",
        "Write your own training function that takes your **model**, an **optimizer**, and a **training criterion**, and iterates over the **training set**.\n",
        "* *Hint*: Because an autoencoder is a form of unsupervised learning, we won't need to use the labels like in the MNIST classification example. Keep in mind the format of the images and whether they're compatible with feed-forward networks.\n",
        "* For each epoch, print and record (in an array or list) the training loss.\n",
        "* You may want to save the model and its weights on file at regular intervals ([checkpointing](https://pytorch.org/tutorials/beginner/saving_loading_models.html#saving-loading-model-for-inference)). In order to visualize the autoencoder's learning process, we suggest to save at least three timepoints: early, intermediate, and final (for instance, if your model converges after 60 epochs, save your model at 5 epochs, 30 epochs, and 60 epochs).\n",
        "\n",
        "A few useful tips:\n",
        "- Feel free to look at the MNIST classification notebook from previous recitations and use it as a template.\n",
        "- Printing out the intermediate variables and their shape at each step can be helpful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "o2vn3O8L6p-2"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, optimizer, criterion,\n",
        "          n_epochs=10, **kwargs):\n",
        "\n",
        "    # Where to run computations (CPU or GPU).\n",
        "    device = kwargs.get(\"device\", torch.device(\"cpu\"))\n",
        "    model.to(device)\n",
        "    \n",
        "    # Optional checkpointing settings (use kwargs to keep call sites simple).\n",
        "    checkpoint_epochs = kwargs.get(\"checkpoint_epochs\", [])\n",
        "    checkpoint_dir = kwargs.get(\"checkpoint_dir\", \"./checkpoints\")\n",
        "    model_name = kwargs.get(\"model_name\", model.__class__.__name__)\n",
        "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "    \n",
        "    # Lists to store per-epoch metrics for plotting later.\n",
        "    losses = []\n",
        "    checkpoints = []\n",
        "\n",
        "    # Loop over epochs (one full pass through the training data each time).\n",
        "    for epoch in range(1, n_epochs + 1):\n",
        "        # Set the model to training mode.\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        n_batches = 0\n",
        "\n",
        "        # Loop over mini-batches from the DataLoader.\n",
        "        for batch in train_loader:\n",
        "            # train_loader may yield (imgs, labels) or imgs directly.\n",
        "            if isinstance(batch, (list, tuple)) and len(batch) >= 1:\n",
        "                imgs = batch[0]\n",
        "            else:\n",
        "                imgs = batch\n",
        "\n",
        "            # Move data to the selected device (CPU/GPU).\n",
        "            imgs = imgs.to(device)\n",
        "\n",
        "            # If you have a helper prepare_batch function you can still use it,\n",
        "            # otherwise the line below is optional and can be omitted.\n",
        "            # imgs = prepare_batch(imgs, model)\n",
        "\n",
        "            # 1) Clear old gradients from the previous step.\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # 2) Forward pass: model may return recon or (recon, z)\n",
        "            out = model(imgs)\n",
        "            if isinstance(out, (list, tuple)):\n",
        "                recon = out[0]\n",
        "            else:\n",
        "                recon = out\n",
        "\n",
        "            # Ensure recon and imgs have compatible shapes for the loss:\n",
        "            # common cases: imgs [B,1,28,28] and recon [B,1,28,28] OR recon [B,784]\n",
        "            if recon.dim() == 2 and imgs.dim() == 4:\n",
        "                # flatten imgs to [B, 784] if recon is flat\n",
        "                target = imgs.view(imgs.size(0), -1)\n",
        "            elif recon.dim() == 4 and imgs.dim() == 2:\n",
        "                # flatten recon (rare) to match imgs\n",
        "                recon = recon.view(recon.size(0), -1)\n",
        "                target = imgs\n",
        "            else:\n",
        "                target = imgs\n",
        "\n",
        "            # 3) Compute loss (reconstruction)\n",
        "            loss = criterion(recon, target)\n",
        "\n",
        "            # 4) Backward pass and update\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Accumulate loss for reporting.\n",
        "            running_loss += loss.item()\n",
        "            n_batches += 1\n",
        "\n",
        "        # Average loss over the epoch.\n",
        "        epoch_loss = running_loss / max(1, n_batches)\n",
        "        losses.append(epoch_loss)\n",
        "\n",
        "        # Optionally save checkpoints at requested epochs.\n",
        "        if epoch in checkpoint_epochs:\n",
        "            ckpt_path = os.path.join(\n",
        "                checkpoint_dir, f\"{model_name}_epoch{epoch}.pt\")\n",
        "            torch.save(model.state_dict(), ckpt_path)\n",
        "            checkpoints.append(ckpt_path)\n",
        "\n",
        "        # Print epoch summary.\n",
        "        print(f\"Epoch {epoch}/{n_epochs} - loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    return losses, checkpoints"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UsMeeItp6p-6"
      },
      "source": [
        "### 5. Define your optimization and evaluation criterion\n",
        "\n",
        "Define an optimizer and criterion (loss function) for your neural network training. To setup your optimizer, you'll have to instantiate your models above, and choose a learning rate. Try a few different optimizers and learning rates to get a sense of what will train within a reasonable timeframe (if your deep network isn't too deep, reaching convergence shouldn't take more than 5-10 minutes with the right choice of learning rate and optimizer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1WQBWlyJcj7w"
      },
      "source": [
        "> **Q5.1) What loss function is suited to this problem?**\n",
        "    MSELoss is suitable because this is a reconstruction task where the goal is to minimize the pixel-wise difference between the original image and the reconstructed image. MSE assumes errors follow Gaussian distribution (Gaussian noise). Since the task is essentially a regression problem, MSE provides a natural measure of reconstruction error.\n",
        "...\n",
        "\n",
        "> **Q5.2) Try a few optimizers, what seemed to work best?**\n",
        "    Adam. \n",
        "...\n",
        "\n",
        "> **Q5.3) What's the effect of choosing different batch sizes?**\n",
        "    Smaller batch sizes introduce more noise in the gradient estimates, which can slow down convergence but often improve generalization because the noise helps the model escape sharp local minima.\n",
        "    Larger batch sizes produce more stable and accurate gradient estimates, leading to faster and smoother training, but they may result in poorer generalization and require more memory.\n",
        "..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "ftxeVJJP6p-7"
      },
      "outputs": [],
      "source": [
        "### Instantiate your model HNet, MyNet\n",
        "model = HNet()\n",
        "### Define your loss function (training criterion)\n",
        "criterion = nn.MSELoss()\n",
        "### Choose your optimizer\n",
        "learning_rate = 1e-4\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zudx4FKI6p_C"
      },
      "source": [
        "### 6. Run your training loop\n",
        "\n",
        "It's a great idea to monitor the early epochs of your training (\"babysit your training\") to keep an eye on learning. Does the learning rate seem too high? too low?\n",
        "\n",
        "(**Hint: it's recommended that you just test a single epoch at a time while you write your training function, to debug and make sure everything is working appropriately.**)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "J87zUaXX6p_D"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/100 - loss: 0.612047\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/100 - loss: 0.512805\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/100 - loss: 0.475190\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/100 - loss: 0.451368\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/100 - loss: 0.437148\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/100 - loss: 0.427830\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 7/100 - loss: 0.419992\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 8/100 - loss: 0.414035\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 9/100 - loss: 0.408851\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n",
            "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
            "libc++abi: terminating due to uncaught exception of type std::__1::system_error: Broken pipe\n",
            "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x12fef27a0>\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1604, in __del__\n",
            "    self._shutdown_workers()\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 1568, in _shutdown_workers\n",
            "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/multiprocessing/process.py\", line 149, in join\n",
            "    res = self._popen.wait(timeout)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_fork.py\", line 40, in wait\n",
            "    if not wait([self.sentinel], timeout):\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/multiprocessing/connection.py\", line 923, in wait\n",
            "    with _WaitSelector() as selector:\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/selectors.py\", line 203, in __exit__\n",
            "    def __exit__(self, *args):\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
            "    _error_if_any_worker_fails()\n",
            "RuntimeError: DataLoader worker (pid 99217) is killed by signal: Abort trap: 6. \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Unexpected exception formatting exception. Falling back to standard exception\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3579, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"/var/folders/qr/fmv1f0ns231f6v1qt881ysb40000gn/T/ipykernel_99128/1011974175.py\", line 3, in <module>\n",
            "    losses, ckpts = train(\n",
            "  File \"/var/folders/qr/fmv1f0ns231f6v1qt881ysb40000gn/T/ipykernel_99128/3544226400.py\", line 41, in train\n",
            "    optimizer.zero_grad()\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/_compile.py\", line 32, in inner\n",
            "    return disable_fn(*args, **kwargs)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py\", line 632, in _fn\n",
            "    return fn(*args, **kwargs)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/optim/optimizer.py\", line 947, in zero_grad\n",
            "    with torch.autograd.profiler.record_function(self._zero_grad_profile_name):\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/autograd/profiler.py\", line 733, in __enter__\n",
            "    self.record = torch.ops.profiler._record_function_enter_new(\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/_ops.py\", line 1116, in __call__\n",
            "    return self._op(*args, **(kwargs or {}))\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2170, in showtraceback\n",
            "    stb = self.InteractiveTB.structured_traceback(\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1457, in structured_traceback\n",
            "    return FormattedTB.structured_traceback(\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1348, in structured_traceback\n",
            "    return VerboseTB.structured_traceback(\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1195, in structured_traceback\n",
            "    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1085, in format_exception_as_a_whole\n",
            "    self.get_records(etb, number_of_lines_of_context, tb_offset) if etb else []\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/IPython/core/ultratb.py\", line 1182, in get_records\n",
            "    res = list(stack_data.FrameInfo.stack_data(etb, options=options))[tb_offset:]\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/stack_data/core.py\", line 597, in stack_data\n",
            "    yield from collapse_repeated(\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/stack_data/utils.py\", line 83, in collapse_repeated\n",
            "    yield from map(mapper, original_group)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/stack_data/core.py\", line 587, in mapper\n",
            "    return cls(f, options)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/stack_data/core.py\", line 551, in __init__\n",
            "    self.executing = Source.executing(frame_or_tb)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/executing/executing.py\", line 224, in executing\n",
            "    source = cls.for_frame(frame)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/executing/executing.py\", line 143, in for_frame\n",
            "    return cls.for_filename(frame.f_code.co_filename, frame.f_globals or {}, use_cache)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/executing/executing.py\", line 172, in for_filename\n",
            "    return cls._for_filename_and_lines(filename, tuple(lines))\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/executing/executing.py\", line 183, in _for_filename_and_lines\n",
            "    result = source_cache[(filename, lines)] = cls(filename, lines)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/executing/executing.py\", line 129, in __init__\n",
            "    cast(EnhancedAST, child).parent = cast(EnhancedAST, node)\n",
            "  File \"/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/_utils/signal_handling.py\", line 73, in handler\n",
            "    _error_if_any_worker_fails()\n",
            "RuntimeError: DataLoader worker (pid 99157) is killed by signal: Abort trap: 6. \n"
          ]
        }
      ],
      "source": [
        "### Set a number of training epochs and train your model.\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "losses, ckpts = train(\n",
        "    train_loader,\n",
        "    model,\n",
        "    optimizer,\n",
        "    criterion,\n",
        "    n_epochs=100,\n",
        "    device=device,\n",
        "    checkpoint_epochs=[],   # æˆ– [1] ä¿å­˜ä¸€æ¬¡æƒé‡ä¾¿äºŽæ£€æŸ¥\n",
        "    checkpoint_dir=\"./checkpoints\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IoOJQt6vFADr"
      },
      "source": [
        "In your training loop, we requested that you store your training loss for each epoch. Using Matplotlib, please plot your training loss as a function of epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "len(losses) = 100\n",
            "losses: [0.5991978695524781, 0.4974461031366767, 0.46432624010643214, 0.44514828990263217, 0.43221903940253675, 0.423311991287447, 0.41679399735383643, 0.4114793171760624, 0.4073731327361898, 0.4030591658056418, 0.39977968145789367, 0.3969757023142345, 0.39498444279628014, 0.3913337803725749, 0.3886945972691721, 0.3862739700371268, 0.3837958076106968, 0.3825242556234413, 0.38108042854744234, 0.37933628207076586, 0.377606083589322, 0.37632872771098413, 0.37531601798051456, 0.37285778758876614, 0.37202898335101, 0.371597091526365, 0.37011670939195385, 0.36913056630315555, 0.36718846771762825, 0.3655354431443123, 0.36544707067993915, 0.3652277851918105, 0.36490343997219227, 0.3636308295259089, 0.3615127547082108, 0.3603366428473865, 0.36019913327973535, 0.35981548016767767, 0.3595838605849219, 0.35845767574777987, 0.35775794651208404, 0.35818676564739205, 0.3557293242864263, 0.35472001507084, 0.35468499689722366, 0.3544895254981035, 0.3536289238980584, 0.3531934472162332, 0.3534740725559975, 0.3525815834241635, 0.35295862734699046, 0.35163862390050504, 0.3510461007989546, 0.35029017906199134, 0.34976358918238803, 0.3492514689339758, 0.3480623424180281, 0.34715651951110693, 0.34955453256300006, 0.3488748980991876, 0.347858543906893, 0.34674363013015375, 0.34673288165887534, 0.3453524830753107, 0.3464795035848231, 0.34590347476605415, 0.34435926882951245, 0.34354908564197484, 0.34361194617458496, 0.3447934605800775, 0.34435165989627714, 0.34338911776857844, 0.34246561058294545, 0.3418056064450156, 0.34179534891775165, 0.3418627842021649, 0.34113121236056915, 0.3410754344229505, 0.3406420089542739, 0.34044999220986355, 0.3395374538674792, 0.33908973980559975, 0.33903635081960193, 0.3389715923429298, 0.3385865748691152, 0.3389769057348085, 0.33830109473738845, 0.3375605379721757, 0.3383106848578463, 0.33741274857317716, 0.33615329175361436, 0.3358898080869524, 0.3375960844539122, 0.336613847756945, 0.33562869265643774, 0.3355598611109801, 0.3336916337770694, 0.3339128501888023, 0.3363784460116551, 0.33540825224888604]\n"
          ]
        }
      ],
      "source": [
        "# å¦‚æžœ losses åœ¨å½“å‰ scope å­˜åœ¨ï¼Œæ‰“å°å®ƒ\n",
        "try:\n",
        "    print(\"len(losses) =\", len(losses))\n",
        "    print(\"losses:\", losses)\n",
        "except NameError:\n",
        "    print(\"losses å˜é‡ä¸å­˜åœ¨ï¼ˆå°šæœªè¢«å®šä¹‰ï¼‰\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xycLVQK-jwiB"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Plot loss curve using Matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(range(1, len(losses)+1), losses)\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Training Loss\")\n",
        "plt.title(\"Training Loss vs Epoch\")\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XitAd61L6p_M"
      },
      "source": [
        "> **Q6.1)  How do you know when your network is done training?**\n",
        "\n",
        "...\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxomi9w5kxIx"
      },
      "source": [
        "Another way to check if your models (`HNet` and `MyNet`) are well trained is to plot a few image reconstructions to see how well your models do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pjS79M0oDj_"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/io/image.py:14: UserWarning: Failed to load image Python extension: 'dlopen(/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so, 0x0006): Library not loaded: @rpath/libjpeg.9.dylib\n",
            "  Referenced from: <0B7EB158-53DC-3403-8A49-22178CAB4612> /Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/image.so\n",
            "  Reason: tried: '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/site-packages/torchvision/../../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/lib/python3.10/lib-dynload/../../libjpeg.9.dylib' (no such file), '/Users/zhwu_cecilia/miniconda3/envs/homework/bin/../lib/libjpeg.9.dylib' (no such file)'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "# extract 6 figures from training DataLoader\n",
        "mini_batch, _ = next(iter(train_loader))\n",
        "n_examples = min(6, mini_batch.shape[0])\n",
        "examples = mini_batch[:n_examples]\n",
        "\n",
        "with torch.no_grad():\n",
        "    reconstr_examples, _ = model(\n",
        "        examples.view(n_examples, -1).to(device))\n",
        "\n",
        "comparison = torch.cat([\n",
        "    examples,\n",
        "    reconstr_examples.view(-1, 1, 28, 28).cpu()\n",
        "])\n",
        "\n",
        "save_image(comparison, 'training_reconstruction.png', nrow=n_examples)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0XG_F4XTqwk9"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "execution_count": 38,
          "metadata": {
            "image/png": {
              "width": 300
            }
          },
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Image('training_reconstruction.png', width=300)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxNZwHPUGxKu"
      },
      "source": [
        "> **Q6.2) What does `torch.no_grad()` do?**\n",
        "    Context-manager that disables gradient calculation. Useful for inference when no backward propagation is needed.\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FC8PXHyR6p_Q"
      },
      "source": [
        "### 7. Visualize the learning process\n",
        "\n",
        "We'll next try to visualize how well the model is learning on the **test set**. To do this, we'll first visualize the \"learning process\" by viewing reconstruction at various stages.\n",
        "\n",
        "* Using your checkpoints saved during training, plot a batch of images from the test set and their corresponding reconstructions based on each of your saved models over time. You should see the quality of the reconstructions improving over time.\n",
        "* To visualize images, you can use the helper functions provided below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "LDO9E6Vl6p_N"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[20], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m reconstruction\u001b[38;5;241m.\u001b[39mreshape(batch\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m],\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m28\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;66;03m# Get a batch of training data\u001b[39;00m\n\u001b[0;32m---> 27\u001b[0m batch, classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Make a grid from batch\u001b[39;00m\n\u001b[1;32m     30\u001b[0m out \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(batch)\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:607\u001b[0m, in \u001b[0;36mreduce_storage\u001b[0;34m(storage)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pickle meta storage; try pickling a meta tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_sharing_strategy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 607\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    609\u001b[0m     rebuild \u001b[38;5;241m=\u001b[39m rebuild_storage_filename\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/storage.py:437\u001b[0m, in \u001b[0;36m_share_memory_lock_protected.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# we can now release it and free the entry.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m to_free \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;66;03m# the data_ptr did.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/storage.py:516\u001b[0m, in \u001b[0;36mUntypedStorage._share_filename_cpu_\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;129m@_share_memory_lock_protected\u001b[39m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_share_filename_cpu_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Broken pipe"
          ]
        }
      ],
      "source": [
        "### Helper Functions for Plotting Multiple Images\n",
        "\n",
        "def imshow(inp,\n",
        "           figsize=(10,10),\n",
        "           mean=0.1307, # for MNIST train\n",
        "           std=0.3081, # for MNIST train\n",
        "           title=None):\n",
        "    \"\"\"Imshow for Tensor.\"\"\"\n",
        "    inp = inp.cpu().detach()\n",
        "    inp = inp.numpy().transpose((1, 2, 0))\n",
        "    mean = np.array(mean)\n",
        "    std = np.array(std)\n",
        "    inp = std * inp + mean\n",
        "    inp = np.clip(inp, 0, 1)\n",
        "    plt.figure(figsize=figsize)\n",
        "    plt.imshow(inp)\n",
        "    if title is not None:\n",
        "        plt.title(title)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
        "\n",
        "def reconstructions_from_batch(model, batch):\n",
        "    batch = batch.view(-1, 28 * 28).to(device)\n",
        "    reconstruction = model(batch)\n",
        "    return reconstruction.reshape(batch.shape[0],1,28,28)\n",
        "\n",
        "# Get a batch of training data\n",
        "batch, classes = next(iter(test_loader))\n",
        "\n",
        "# Make a grid from batch\n",
        "out = torchvision.utils.make_grid(batch)\n",
        "imshow(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "E2edUEVG3Rev"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "Broken pipe",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[19], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m### Iterate over checkpoints and plot reconstruction\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m### figures from the test set.\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m batch, classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      4\u001b[0m out_original \u001b[38;5;241m=\u001b[39m torchvision\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mmake_grid(batch)\n\u001b[1;32m      5\u001b[0m imshow(out_original, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Images (Ground Truth)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py:484\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py:415\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 415\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1138\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1131\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1138\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/context.py:288\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 288\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/popen_spawn_posix.py:47\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m     reduction\u001b[38;5;241m.\u001b[39mdump(prep_data, fp)\n\u001b[0;32m---> 47\u001b[0m     \u001b[43mreduction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     set_spawning_popen(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/multiprocessing/reduction.py:60\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, file, protocol)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mdump\u001b[39m(obj, file, protocol\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m     59\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m'''Replacement for pickle.dump() using ForkingPickler.'''\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m     \u001b[43mForkingPickler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprotocol\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:607\u001b[0m, in \u001b[0;36mreduce_storage\u001b[0;34m(storage)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    604\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot pickle meta storage; try pickling a meta tensor instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m get_sharing_strategy() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfile_system\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 607\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m \u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    608\u001b[0m     cache_key \u001b[38;5;241m=\u001b[39m metadata[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    609\u001b[0m     rebuild \u001b[38;5;241m=\u001b[39m rebuild_storage_filename\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/storage.py:437\u001b[0m, in \u001b[0;36m_share_memory_lock_protected.<locals>.wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    434\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 437\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;66;03m# If we acquired the storage lock here and we're done working on it\u001b[39;00m\n\u001b[1;32m    440\u001b[0m     \u001b[38;5;66;03m# we can now release it and free the entry.\u001b[39;00m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m to_free \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m         \u001b[38;5;66;03m# Ensure that the cdata from the storage didn't change and only\u001b[39;00m\n\u001b[1;32m    443\u001b[0m         \u001b[38;5;66;03m# the data_ptr did.\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/envs/homework/lib/python3.10/site-packages/torch/storage.py:516\u001b[0m, in \u001b[0;36mUntypedStorage._share_filename_cpu_\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m \u001b[38;5;129m@_share_memory_lock_protected\u001b[39m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_share_filename_cpu_\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_share_filename_cpu_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Broken pipe"
          ]
        }
      ],
      "source": [
        "### Iterate over checkpoints and plot reconstruction\n",
        "### figures from the test set.\n",
        "batch, classes = next(iter(test_loader))\n",
        "out_original = torchvision.utils.make_grid(batch)\n",
        "imshow(out_original, title=\"Original Images (Ground Truth)\")\n",
        "\n",
        "epochs_saved = sorted(list(checkpoints.keys()))\n",
        "for epoch in epochs_saved:\n",
        "    ckpt_path = checkpoints[epoch]\n",
        "    eval_model = model().to(device)\n",
        "    eval_model.load_state_dict(torch.load(ckpt_path))\n",
        "    eval_model.eval()\n",
        "    with torch.no_grad():\n",
        "\n",
        "            reconstructed_batch = reconstructions_from_batch(eval_model, batch)\n",
        "\n",
        "            out_reconstructed = torchvision.utils.make_grid(reconstructed_batch)\n",
        "\n",
        "            imshow(out_reconstructed, title=f\"Reconstructed Images - Epoch {epoch}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dfhuSg4D6p_R"
      },
      "source": [
        "### 8. Visualize the latent space\n",
        "\n",
        "As discussed in class, the first half of an autoencoder (the *encoder*) maps the original input into a lower-dimensional latent space.\n",
        "* Just as shown in Hinton and Salakhutdinov, run your test set of 10,000 MNIST digits through the **encoding layer** of one of the trained networks above. Each sample should readily map to a 2-dimension point. To do this, it will be helpful to fill out a new function, **encode** below, that takes in your trained model and the `test_dataloader` to produce 2d latent embeddings and their corresponding labels.\n",
        "* Plot each point in these two dimensions, and color each point in this **latent space** by their known **labels**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "PNg0PfrK4gzn"
      },
      "outputs": [],
      "source": [
        "### Write a helper function to grab examples from the test_loader to generate\n",
        "### pairs of embeddings and their associated labels\n",
        "\n",
        "def get_encodings(model, device, test_loader):\n",
        "  #### Fill this in! ####\n",
        "  latent_embeddings = None # get the latent embeddings from the bottleneck\n",
        "  labels = None # each pair of coordinates has an associated label\n",
        "  return latent_embeddings, labels\n",
        "\n",
        "### Plot 2D latent space representation color-coded according to their true labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IuGkujKgGPEi"
      },
      "source": [
        "> **Q8.1) Does your autoencoder separate out different classes effectively? What classes seem to be closer and what classes are farther apart in this latent space?**\n",
        "\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78___-BU6p_S"
      },
      "source": [
        "## Optional (advanced): Train an autoencoder on CelebA Faces\n",
        "\n",
        "Real-world images tend to be far more complex than digits from MNIST. As an optional exercise for your own interest, or for students looking for more experience, we'll investigate a subset of CelebA below.\n",
        "\n",
        "We provide the images in a .zip file (`faces.zip`) in the class's Google Drive folder, which contains a \"train\" and \"test\" set of 80k and 10k images, respectively. Although these are color, RGB images, below we've set up the datasets to convert these to grayscale with precomputed means (0.4401) and stds (0.2407), for convenience and easier compute."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "1K6o1Wr88EXj"
      },
      "outputs": [],
      "source": [
        "### Download faces.zip and unzip it into bmi219_downloads/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "nM7WXmAu6p_T"
      },
      "outputs": [],
      "source": [
        "# preprocessing = transforms.Compose([\n",
        "#     transforms.Grayscale(),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize((0.4401,), (0.2407,)),\n",
        "# ])\n",
        "\n",
        "# train_dataset = datasets.ImageFolder(\n",
        "#     'bmi212_downloads/Faces/train',\n",
        "#     transform=preprocessing)\n",
        "\n",
        "# test_dataset = datasets.ImageFolder(\n",
        "#     'bmi212_downloads/Faces/test',\n",
        "#     transform=preprocessing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQn4cgBW6p_X"
      },
      "source": [
        "As above, you'll want to:\n",
        "\n",
        "1. set up your dataloaders and visualize some of the images\n",
        "2. set up your autoencoder network architecture\n",
        "3. define your training criterion and optimizer\n",
        "4. train your network\n",
        "    \n",
        "In this case, you should be able to reuse much of your code from above. Consider a few questions:\n",
        "\n",
        "1. How well do complex images like faces work with a latent dimension of 2?\n",
        "2. Do reconstructions look better with a larger bottleneck?\n",
        "3. What kind of features are poorly reconstructed? What happens to sunglasses, hats, and hands?\n",
        "4. Try sampling the 2-d latent space close to existing examples (by adding some noise...) or randomly. What do the generated images look like?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "homework",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
